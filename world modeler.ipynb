{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŒ Autonomous World Modeler\n",
        "\n",
        "**Simulate alternate futures using generative AI, real-time data retrieval, and interactive scenario control.**\n",
        "\n",
        "---\n",
        "\n",
        "##  Project Overview\n",
        "\n",
        "Autonomous World Modeler 3.0 is an interactive AI system that allows users to explore alternate futures by:\n",
        "\n",
        "- Entering custom **\"what-if\" scenarios** (e.g., \"AGI is achieved by 2040\")\n",
        "- Adjusting **policy levers** (like tech adoption, regulation, and climate investment)\n",
        "- Integrating **live news** and curated data to ground scenario predictions\n",
        "- Using a **local LLM (Mistral-7B Instruct)** to generate rich, structured futures\n",
        "- Extracting and visualizing **quantitative metrics** like probability, impact, and sentiment\n",
        "- Producing **professional reports** in PDF format with scenario details and comparisons\n",
        "\n",
        "---\n",
        "\n",
        "##  Key Capabilities\n",
        "-  Counterfactual modeling with dynamic sliders\n",
        "-  RAG (Retrieval-Augmented Generation) with semantic relevance\n",
        "-  Inline scenario analysis with structured prompts\n",
        "-  Visual and PDF reporting of simulation results\n",
        "-  Error-resilient and well-logged workflow\n",
        "\n",
        "---\n",
        "\n",
        "##  Intended Use Cases\n",
        "- Foresight analysis and speculative design\n",
        "- Policy and economic simulations\n",
        "- Educational or training scenarios\n",
        "- AI model testing in sandboxed futures\n",
        "\n",
        "---\n",
        "\n",
        ">  Built with `Gradio`, `ctransformers`, `sentence-transformers`, and `matplotlib`.  \n",
        ">  Designed to run in Google Colab\n",
        "\n"
      ],
      "metadata": {
        "id": "bc5G9OIWJmub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Environment Setup: Install Required Python Packages\n",
        "\n",
        "Before running the app, install the core dependencies using `pip`. These packages support:\n",
        "\n",
        "- **Gradio** â€“ UI interface for scenario input/output\n",
        "- **ctransformers** â€“ Lightweight LLM inference for GGUF models\n",
        "- **fpdf** â€“ PDF generation for scenario reports\n",
        "- **sentence-transformers** â€“ Semantic similarity for RAG\n",
        "- **scikit-learn** â€“ Cosine similarity computation\n",
        "- **matplotlib** â€“ Visualization of scenario comparisons\n",
        "\n",
        "```bash\n",
        "!pip install gradio ctransformers fpdf sentence-transformers scikit-learn matplotlib\n"
      ],
      "metadata": {
        "id": "hycSM4SXJMX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install gradio ctransformers fpdf sentence-transformers scikit-learn matplotlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xecEhF8hheEa",
        "outputId": "299648d7-5939-424b-a53d-bcf8807daaa4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Collecting ctransformers\n",
            "  Downloading ctransformers-0.2.27-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.13)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from ctransformers) (9.0.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.52.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading ctransformers-0.2.27-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=651571badf5a1a91aa0d47b1453ee04607309fda6993989bd08afed4a088aea5\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/66/bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ctransformers\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed ctransformers-0.2.27 fpdf-1.7.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Model Download: Mistral-7B Instruct (GGUF Format)\n",
        "\n",
        "This command downloads the **quantized Mistral-7B-Instruct model** (`Q4_K_M` version) from Hugging Face. It is required by `ctransformers` to run local inference efficiently on CPU or GPU.\n",
        "You can use a different model as per your requirements\n",
        "or even use the chatgpt api with modifications in the code.\n",
        "\n",
        "```bash\n",
        "!wget -O mistral-7b-instruct-v0.1.Q4_K_M.gguf https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf\n"
      ],
      "metadata": {
        "id": "BkoWAr9fI4-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O mistral-7b-instruct-v0.1.Q4_K_M.gguf https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SyIs4_viGMW",
        "outputId": "4152a224-0a01-4699-8627-5ca95fa5ebde"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-01 08:54:04--  https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.169.137.111, 3.169.137.119, 3.169.137.19, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.169.137.111|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/46/12/46124cd8d4788fd8e0879883abfc473f247664b987955cc98a08658f7df6b826/14466f9d658bf4a79f96c3f3f22759707c291cac4e62fea625e80c7d32169991?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.1.Q4_K_M.gguf%3B+filename%3D%22mistral-7b-instruct-v0.1.Q4_K_M.gguf%22%3B&Expires=1751363644&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTM2MzY0NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy80Ni8xMi80NjEyNGNkOGQ0Nzg4ZmQ4ZTA4Nzk4ODNhYmZjNDczZjI0NzY2NGI5ODc5NTVjYzk4YTA4NjU4ZjdkZjZiODI2LzE0NDY2ZjlkNjU4YmY0YTc5Zjk2YzNmM2YyMjc1OTcwN2MyOTFjYWM0ZTYyZmVhNjI1ZTgwYzdkMzIxNjk5OTE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=jkpKYkhfvYZJQiXSNUmWkpezrszPoYuhqAwhe6Cl5PWsPZsJrjTw3w9Ygiew40N3x82M7mybd7X0K%7Et7IN8L9Vi3fTF0iyQQHXAzB26BJzCg0w4O0wHbci7DrA-veB8gaaGlsLpj8kgJyutAdwQeosmMprEk406cynkIpVZJ4bZ%7EcDUFU2V%7Ed1%7EE7O0c%7Elal9icU9dM0TZ7j%7EPQbR2tlkhOQCgQj2rMg2UObyXLMJ7sfJ05GMmzxyM5yXzm2ImFPM4ud6slBSvn26mkFFIj%7Efir5zDrSS9sd6gndUxsQ%7ElANRh6ezqQxsVFykNlqI3oaY5NW-UV4Za1i0PMKq3hG2w__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2025-07-01 08:54:04--  https://cdn-lfs.hf.co/repos/46/12/46124cd8d4788fd8e0879883abfc473f247664b987955cc98a08658f7df6b826/14466f9d658bf4a79f96c3f3f22759707c291cac4e62fea625e80c7d32169991?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.1.Q4_K_M.gguf%3B+filename%3D%22mistral-7b-instruct-v0.1.Q4_K_M.gguf%22%3B&Expires=1751363644&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTM2MzY0NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy80Ni8xMi80NjEyNGNkOGQ0Nzg4ZmQ4ZTA4Nzk4ODNhYmZjNDczZjI0NzY2NGI5ODc5NTVjYzk4YTA4NjU4ZjdkZjZiODI2LzE0NDY2ZjlkNjU4YmY0YTc5Zjk2YzNmM2YyMjc1OTcwN2MyOTFjYWM0ZTYyZmVhNjI1ZTgwYzdkMzIxNjk5OTE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=jkpKYkhfvYZJQiXSNUmWkpezrszPoYuhqAwhe6Cl5PWsPZsJrjTw3w9Ygiew40N3x82M7mybd7X0K%7Et7IN8L9Vi3fTF0iyQQHXAzB26BJzCg0w4O0wHbci7DrA-veB8gaaGlsLpj8kgJyutAdwQeosmMprEk406cynkIpVZJ4bZ%7EcDUFU2V%7Ed1%7EE7O0c%7Elal9icU9dM0TZ7j%7EPQbR2tlkhOQCgQj2rMg2UObyXLMJ7sfJ05GMmzxyM5yXzm2ImFPM4ud6slBSvn26mkFFIj%7Efir5zDrSS9sd6gndUxsQ%7ElANRh6ezqQxsVFykNlqI3oaY5NW-UV4Za1i0PMKq3hG2w__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 13.226.61.33, 13.226.61.68, 13.226.61.49, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|13.226.61.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4368438944 (4.1G) [binary/octet-stream]\n",
            "Saving to: â€˜mistral-7b-instruct-v0.1.Q4_K_M.ggufâ€™\n",
            "\n",
            "mistral-7b-instruct 100%[===================>]   4.07G   216MB/s    in 34s     \n",
            "\n",
            "2025-07-01 08:54:38 (123 MB/s) - â€˜mistral-7b-instruct-v0.1.Q4_K_M.ggufâ€™ saved [4368438944/4368438944]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Autonomous World Modeler 3.0**\n",
        "\n",
        "A generative AI system that simulates alternate futures using live data, structured analysis, and counterfactual scenarios.\n",
        "\n",
        "---\n",
        "\n",
        "##  Key Features\n",
        "- **Dynamic UI** built with Gradio for interactive scenario input\n",
        "- **Retrieval-Augmented Generation (RAG)** using live news and static knowledge\n",
        "- **Structured Prompting** for consistent and evaluable outputs\n",
        "- **PDF Report Generator** with metrics and scenario breakdowns\n",
        "- **Visualization Module** to compare future scenarios\n",
        "- **Error Logging & Resilience** via `logging` and `try/except` blocks\n",
        "\n",
        "---\n",
        "\n",
        "##  Modules Overview\n",
        "\n",
        "- `Config`: Centralized configuration for model, sliders, and keys.\n",
        "- `LiveDataRetriever`: Pulls live news and filters relevant context using semantic similarity.\n",
        "- `FutureEvaluator`: Extracts structured metrics (probability, impact, timeframe, sentiment) from LLM outputs.\n",
        "- `PDFReport`: Creates a professional-grade PDF summarizing each scenario and its evaluated metrics.\n",
        "- `apply_counterfactuals`: Modifies scenario narrative based on user-controlled policy sliders.\n",
        "- `generate_visualization`: Builds a bar chart comparing scenario probabilities and impacts.\n",
        "- `create_pdf_report`: Assembles the scenario and futures into a formatted PDF.\n",
        "- `parse_llm_response`: Parses raw LLM output using JSON or regex fallback.\n",
        "- `format_display_output`: Builds a readable summary of scenario outcomes.\n",
        "- `create_interface`: Gradio UI integrating all system components.\n",
        "\n",
        "---\n",
        "\n",
        "##  How It Works\n",
        "\n",
        "1. User enters a base scenario and (optionally) selects real-time news grounding.\n",
        "2. Policy sliders modify scenario assumptions (e.g., tech adoption, regulation).\n",
        "3. Model generates three plausible futures with contextual grounding.\n",
        "4. Each future is parsed and evaluated for key metrics.\n",
        "5. Outputs include:\n",
        "   - Visual chart comparing future probabilities\n",
        "   - Full-text analysis\n",
        "   - Downloadable PDF report\n",
        "\n",
        "---\n",
        "\n",
        "##  Requirements\n",
        "- `gradio`\n",
        "- `ctransformers`\n",
        "- `sentence-transformers`\n",
        "- `scikit-learn`\n",
        "- `matplotlib`\n",
        "- `fpdf`\n",
        "- `requests`, `re`, `json`, `datetime`, `logging`\n",
        "\n",
        ">  **Note:** You'll need a NewsAPI.org key for live news to work.\n",
        "\n",
        "---\n",
        "\n",
        "##  Example Scenario\n",
        "> \"AI achieves AGI by 2040 with rapid deployment in healthcare and defense.\"\n",
        "\n",
        "You can test this by pasting it into the UI and observing how various policy assumptions alter the outcomes.\n",
        "\n",
        "---\n",
        "\n",
        "##  Logging\n",
        "All logs are written to `world_modeler.log` for debugging and traceability.\n"
      ],
      "metadata": {
        "id": "DJyDjzrPILWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "AUTONOMOUS WORLD MODELER 3.0\n",
        "Generative AI to Simulate Alternate Futures with:\n",
        "- Dynamic UI controls\n",
        "- Robust error handling\n",
        "- Structured data extraction\n",
        "- Professional reporting\n",
        "\"\"\"\n",
        "\n",
        "import gradio as gr\n",
        "from ctransformers import AutoModelForCausalLM\n",
        "from fpdf import FPDF\n",
        "import requests\n",
        "import re\n",
        "import logging\n",
        "import json\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Optional\n",
        "import matplotlib.pyplot as plt\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# ====================== CONFIGURATION ======================\n",
        "class Config:\n",
        "    # Project constants and configuration parameters\n",
        "    PROJECT_TITLE = \"Autonomous World Modeler: Generative AI to Simulate Alternate Futures\"\n",
        "    MODEL_PATH = \"mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
        "    MODEL_TYPE = \"mistral\"\n",
        "    RAG_THRESHOLD = 0.7  # Threshold for retrieval-augmented generation filtering\n",
        "    POLICY_SLIDERS = {\n",
        "        \"tech_adoption\": (0, 100, 50, \"Technology Adoption Rate (%)\"),\n",
        "        \"regulation\": (0, 10, 5, \"Government Regulation Level\"),\n",
        "        \"climate_investment\": (0, 20, 5, \"Climate Investment ($T/year)\")\n",
        "    }\n",
        "    NEWS_API_KEY = \"your_api_key_here\"  # Insert your actual News API key here\n",
        "    LOG_FILE = \"world_modeler.log\"\n",
        "\n",
        "# ====================== SETUP LOGGING ======================\n",
        "logging.basicConfig(\n",
        "    filename=Config.LOG_FILE,\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ====================== LOAD MODEL ======================\n",
        "try:\n",
        "    logger.info(\"Loading Mistral-7B model...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        Config.MODEL_PATH,\n",
        "        model_type=Config.MODEL_TYPE,\n",
        "        max_new_tokens=1024,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    logger.info(\"Model loaded successfully\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Model loading failed: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# ====================== CORE CLASSES ======================\n",
        "class LiveDataRetriever:\n",
        "    \"\"\"\n",
        "    Retrieves live data and context for grounding the generative model.\n",
        "    Uses SentenceTransformer embeddings for semantic similarity to filter relevant information.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    def fetch_news(self, query: str) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Calls News API to fetch recent articles matching the query.\n",
        "        Returns a list of dicts with text and source.\n",
        "        Handles API errors gracefully.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            url = f\"https://newsapi.org/v2/everything?q={query}&apiKey={Config.NEWS_API_KEY}\"\n",
        "            response = requests.get(url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            articles = response.json().get(\"articles\", [])\n",
        "            return [\n",
        "                {\"text\": article[\"title\"], \"source\": article[\"source\"][\"name\"]}\n",
        "                for article in articles[:3]\n",
        "            ]\n",
        "        except Exception as e:\n",
        "            logger.error(f\"News API failed: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def retrieve_relevant_data(self, query: str, use_news: bool) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Combines static knowledge base with optional live news,\n",
        "        then uses embeddings to return only relevant data with similarity above threshold.\n",
        "        \"\"\"\n",
        "        # Base knowledge data\n",
        "        knowledge_base = [\n",
        "            {\"text\": \"Global AI investment reached $92B in 2023\", \"source\": \"McKinsey 2023\"},\n",
        "            {\"text\": \"Renewable energy accounts for 30% of global electricity\", \"source\": \"IEA 2024\"}\n",
        "        ]\n",
        "\n",
        "        # Append news if enabled\n",
        "        if use_news:\n",
        "            knowledge_base.extend(self.fetch_news(query))\n",
        "\n",
        "        # Encode query and knowledge base texts\n",
        "        query_embed = self.encoder.encode(query)\n",
        "        docs_embed = self.encoder.encode([d[\"text\"] for d in knowledge_base])\n",
        "\n",
        "        # Compute cosine similarity scores\n",
        "        similarities = cosine_similarity([query_embed], docs_embed)[0]\n",
        "\n",
        "        # Filter by threshold and attach score\n",
        "        relevant = [\n",
        "            {**knowledge_base[i], \"score\": float(score)}\n",
        "            for i, score in enumerate(similarities)\n",
        "            if score > Config.RAG_THRESHOLD\n",
        "        ]\n",
        "\n",
        "        return relevant\n",
        "\n",
        "class FutureEvaluator:\n",
        "    \"\"\"\n",
        "    Extracts structured metrics from generated future scenario text,\n",
        "    such as probability, timeframe, impact, and sentiment.\n",
        "    Uses regex and simple sentiment scoring heuristics.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def extract_metrics(text: str) -> Dict:\n",
        "        return {\n",
        "            \"probability\": FutureEvaluator._extract_probability(text),\n",
        "            \"timeframe\": FutureEvaluator._extract_timeframe(text),\n",
        "            \"impact\": FutureEvaluator._extract_impact(text),\n",
        "            \"sentiment\": FutureEvaluator._analyze_sentiment(text)\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def _extract_probability(text: str) -> float:\n",
        "        # Extract probability percentage from text, fallback to 0.5 (50%)\n",
        "        match = re.search(r\"probability:?\\s*(\\d+)%\", text, re.IGNORECASE)\n",
        "        if match:\n",
        "            prob = float(match.group(1)) / 100.0\n",
        "            return min(1.0, max(0.0, prob))\n",
        "        return 0.5\n",
        "\n",
        "    @staticmethod\n",
        "    def _extract_timeframe(text: str) -> str:\n",
        "        # Extract timeframe or timescale from text, fallback to default range\n",
        "        match = re.search(r\"time(?:frame|scale):?\\s*([^\\n]+)\", text, re.IGNORECASE)\n",
        "        return match.group(1).strip() if match else \"2025-2050\"\n",
        "\n",
        "    @staticmethod\n",
        "    def _extract_impact(text: str) -> str:\n",
        "        # Simple categorization of impact level from keywords\n",
        "        lower_text = text.lower()\n",
        "        if \"high impact\" in lower_text or \"significant\" in lower_text:\n",
        "            return \"High\"\n",
        "        elif \"moderate\" in lower_text or \"medium\" in lower_text:\n",
        "            return \"Medium\"\n",
        "        return \"Low\"\n",
        "\n",
        "    @staticmethod\n",
        "    def _analyze_sentiment(text: str) -> float:\n",
        "        # Sentiment score based on counting positive and negative keywords\n",
        "        positive = len(re.findall(r\"\\b(good|benefit|positive|improve)\\b\", text, re.IGNORECASE))\n",
        "        negative = len(re.findall(r\"\\b(bad|harm|negative|worse)\\b\", text, re.IGNORECASE))\n",
        "        total = positive + negative\n",
        "        if total == 0:\n",
        "            return 0.0\n",
        "        return (positive - negative) / total\n",
        "\n",
        "class PDFReport(FPDF):\n",
        "    \"\"\"\n",
        "    Generates a professional PDF report summarizing the scenario and generated futures,\n",
        "    including metrics displayed in a tabular format.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.set_auto_page_break(auto=True, margin=15)\n",
        "\n",
        "    def header(self):\n",
        "        # Report header with title and timestamp\n",
        "        self.set_font('Arial', 'B', 14)\n",
        "        self.cell(0, 10, Config.PROJECT_TITLE, 0, 1, 'C')\n",
        "        self.set_font('Arial', '', 10)\n",
        "        self.cell(0, 5, datetime.now().strftime('%B %d, %Y at %H:%M'), 0, 1, 'C')\n",
        "        self.ln(10)\n",
        "\n",
        "    def add_future_section(self, idx: int, text: str, metrics: Dict):\n",
        "        # Add a section per future scenario, with text and metrics table\n",
        "        self.set_font('Arial', 'B', 12)\n",
        "        self.cell(0, 8, f'Future Scenario #{idx + 1}', 0, 1)\n",
        "        self.set_font('Arial', '', 10)\n",
        "        self.multi_cell(0, 6, text)\n",
        "        self.ln(4)\n",
        "\n",
        "        # Metrics table header\n",
        "        self.set_font('Arial', 'B', 10)\n",
        "        self.cell(40, 8, 'Metric', border=1)\n",
        "        self.cell(0, 8, 'Value', border=1, ln=1)\n",
        "\n",
        "        # Metrics rows\n",
        "        self.set_font('Arial', '', 10)\n",
        "        for metric, value in metrics.items():\n",
        "            self.cell(40, 8, metric.capitalize(), border=1)\n",
        "            self.cell(0, 8, str(value), border=1, ln=1)\n",
        "        self.ln(10)\n",
        "\n",
        "# ====================== WORKFLOW FUNCTIONS ======================\n",
        "def apply_counterfactuals(scenario: str, params: Dict) -> str:\n",
        "    \"\"\"\n",
        "    Applies policy slider values to the base scenario by appending\n",
        "    descriptive modifications reflecting counterfactual conditions.\n",
        "    \"\"\"\n",
        "    modifications = []\n",
        "    if params.get(\"tech_adoption\") is not None:\n",
        "        modifications.append(f\"Technology adoption at {params['tech_adoption']}%\")\n",
        "    if params.get(\"regulation\") is not None:\n",
        "        modifications.append(f\"Regulation level at {params['regulation']}/10\")\n",
        "    if params.get(\"climate_investment\") is not None:\n",
        "        modifications.append(f\"Climate investment at ${params['climate_investment']}T/year\")\n",
        "\n",
        "    if modifications:\n",
        "        return f\"{scenario}\\n\\nModified Conditions:\\n- \" + \"\\n- \".join(modifications)\n",
        "    return scenario\n",
        "\n",
        "def generate_visualization(futures: List[Dict]) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Creates a bar plot comparing the probability of each future,\n",
        "    with alpha transparency representing impact level.\n",
        "    Returns the path to saved image or None on failure.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        probabilities = [f['metrics'].get('probability', 0.5) for f in futures]\n",
        "        # Use alpha to indicate impact: High=0.8, Medium=0.5, Low=0.3\n",
        "        impact_alpha = []\n",
        "        for f in futures:\n",
        "            impact = f['metrics'].get('impact', 'Low')\n",
        "            if impact == \"High\":\n",
        "                impact_alpha.append(0.8)\n",
        "            elif impact == \"Medium\":\n",
        "                impact_alpha.append(0.5)\n",
        "            else:\n",
        "                impact_alpha.append(0.3)\n",
        "\n",
        "        labels = [f\"Future {i + 1}\" for i in range(len(futures))]\n",
        "        bars = plt.bar(labels, probabilities, color=['#4CAF50', '#2196F3', '#FFC107'])\n",
        "        for bar, alpha in zip(bars, impact_alpha):\n",
        "            bar.set_alpha(alpha)\n",
        "\n",
        "        plt.title(\"Future Scenario Comparison\")\n",
        "        plt.ylabel(\"Probability\")\n",
        "        plt.ylim(0, 1)\n",
        "\n",
        "        plot_path = \"futures_plot.png\"\n",
        "        plt.savefig(plot_path, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        return plot_path\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Visualization failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def create_pdf_report(scenario: str, futures: List[Dict]) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Generates a detailed PDF report including the base scenario,\n",
        "    all future scenarios with metrics, and saves it to a file.\n",
        "    Returns the filename or None if generation fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pdf = PDFReport()\n",
        "        pdf.add_page()\n",
        "\n",
        "        # Add main title and scenario description\n",
        "        pdf.set_font('Arial', 'B', 16)\n",
        "        pdf.cell(0, 10, 'Future Scenario Analysis', 0, 1, 'C')\n",
        "        pdf.ln(10)\n",
        "\n",
        "        pdf.set_font('Arial', 'B', 12)\n",
        "        pdf.cell(0, 8, 'Base Scenario:', 0, 1)\n",
        "        pdf.set_font('Arial', '', 10)\n",
        "        pdf.multi_cell(0, 6, scenario)\n",
        "        pdf.ln(15)\n",
        "\n",
        "        # Add all futures with metrics\n",
        "        for i, future in enumerate(futures):\n",
        "            pdf.add_future_section(i, future['text'], future['metrics'])\n",
        "\n",
        "        filename = f\"future_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf\"\n",
        "        pdf.output(filename)\n",
        "        return filename\n",
        "    except Exception as e:\n",
        "        logger.error(f\"PDF generation failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def parse_llm_response(response: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Attempts to parse the LLM's output to extract a list of future scenarios.\n",
        "    Tries JSON extraction first, then falls back to numbered list parsing.\n",
        "    If both fail, returns the raw response truncated.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Attempt to find JSON content in the response\n",
        "        json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
        "        if json_match:\n",
        "            data = json.loads(json_match.group())\n",
        "            if isinstance(data, list):\n",
        "                return data[:3]\n",
        "            elif \"futures\" in data:\n",
        "                return data[\"futures\"][:3]\n",
        "\n",
        "        # Fallback: extract numbered futures from text\n",
        "        futures = re.findall(r\"\\d+\\.\\s+(.*?)(?=\\n\\d+\\.|\\Z)\", response, re.DOTALL)\n",
        "        return [{\"description\": f.strip()} for f in futures if f.strip()][:3]\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Response parsing failed: {str(e)}\")\n",
        "        # Return the raw text truncated as a single future\n",
        "        return [{\"description\": response[:500]}]\n",
        "\n",
        "def format_display_output(scenario: str, futures: List[Dict]) -> str:\n",
        "    \"\"\"\n",
        "    Formats the scenario and futures text for display in the Gradio UI,\n",
        "    including metrics for each future scenario.\n",
        "    \"\"\"\n",
        "    output = [f\"SCENARIO:\\n{scenario}\\n\\n\"]\n",
        "    for i, future in enumerate(futures):\n",
        "        output.append(f\"FUTURE {i + 1}:\\n{future['text']}\\n\")\n",
        "        output.append(\"METRICS:\\n\")\n",
        "        for metric, value in future['metrics'].items():\n",
        "            output.append(f\"  - {metric.capitalize()}: {value}\\n\")\n",
        "        output.append(\"\\n\")\n",
        "    return \"\".join(output)\n",
        "\n",
        "# ====================== GRADIO INTERFACE ======================\n",
        "def create_interface():\n",
        "    with gr.Blocks(title=Config.PROJECT_TITLE, theme=gr.themes.Soft()) as demo:\n",
        "        # Title and description\n",
        "        gr.Markdown(f\"# {Config.PROJECT_TITLE}\")\n",
        "        gr.Markdown(\"Explore alternate futures with counterfactual testing and live data grounding.\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                # Input fields for base scenario and news options\n",
        "                scenario_input = gr.Textbox(\n",
        "                    label=\"Base Scenario\",\n",
        "                    lines=4,\n",
        "                    placeholder=\"Describe a future scenario (e.g., 'AI achieves AGI by 2040')\"\n",
        "                )\n",
        "                use_news = gr.Checkbox(label=\"Include live news context\")\n",
        "                news_topic = gr.Textbox(\n",
        "                    label=\"News search term\",\n",
        "                    visible=False,\n",
        "                    placeholder=\"e.g., 'AI regulation'\"\n",
        "                )\n",
        "                use_rag = gr.Checkbox(label=\"Enable RAG grounding\", value=True)\n",
        "\n",
        "                # Policy sliders (dynamic controls)\n",
        "                sliders = [\n",
        "                    gr.Slider(*v[:3], label=v[3])\n",
        "                    for v in Config.POLICY_SLIDERS.values()\n",
        "                ]\n",
        "\n",
        "                simulate_btn = gr.Button(\"Simulate Futures\", variant=\"primary\")\n",
        "\n",
        "            with gr.Column():\n",
        "                # Output displays: plot and generated text\n",
        "                plot_output = gr.Image(label=\"Scenario Comparison\", width=500)\n",
        "\n",
        "        with gr.Row():\n",
        "            text_output = gr.Textbox(label=\"Generated Futures\", lines=15)\n",
        "            report_output = gr.File(label=\"Download Report\")\n",
        "\n",
        "        # Show/hide news_topic input based on use_news checkbox\n",
        "        use_news.change(\n",
        "            lambda x: gr.update(visible=x),\n",
        "            inputs=use_news,\n",
        "            outputs=news_topic\n",
        "        )\n",
        "\n",
        "        # Main simulation workflow handler\n",
        "        @simulate_btn.click(\n",
        "            inputs=[scenario_input, use_news, news_topic, use_rag, *sliders],\n",
        "            outputs=[text_output, plot_output, report_output]\n",
        "        )\n",
        "        def handle_simulation(scenario, use_news, news_topic, use_rag, *slider_values):\n",
        "            try:\n",
        "                logger.info(\"Starting simulation workflow\")\n",
        "\n",
        "                # 1. Apply policy slider counterfactuals to scenario text\n",
        "                params = dict(zip(Config.POLICY_SLIDERS.keys(), slider_values))\n",
        "                modified_scenario = apply_counterfactuals(scenario, params)\n",
        "\n",
        "                # 2. Retrieve relevant data using either scenario or news topic as query\n",
        "                retriever = LiveDataRetriever()\n",
        "                search_query = news_topic if use_news else modified_scenario\n",
        "                relevant_data = retriever.retrieve_relevant_data(search_query, use_news)\n",
        "\n",
        "                # 3. Compose prompt with context for LLM generation\n",
        "                context_str = \"\\n\".join(f\"- {d['text']} ({d['source']})\" for d in relevant_data)\n",
        "                prompt = f\"\"\"Generate 3 detailed alternate futures for:\n",
        "{modified_scenario}\n",
        "\n",
        "Context:\n",
        "{context_str}\n",
        "\n",
        "For each future, include:\n",
        "1. Probability: XX%\n",
        "2. Timeframe: YYYY-YYYY\n",
        "3. Impact: High/Medium/Low\n",
        "4. Economic Impact\n",
        "5. Population Affected\n",
        "6. Environmental Impact\n",
        "7. Innovation Rate\n",
        "8. Key Drivers\"\"\"\n",
        "\n",
        "                # 4. Call the model to generate futures\n",
        "                llm_response = model(prompt)\n",
        "\n",
        "                # 5. Parse and extract futures from model output\n",
        "                futures = parse_llm_response(llm_response)\n",
        "                if not futures:\n",
        "                    raise ValueError(\"No valid futures generated\")\n",
        "\n",
        "                # 6. Evaluate futures by extracting structured metrics\n",
        "                evaluator = FutureEvaluator()\n",
        "                evaluated_futures = []\n",
        "                for future in futures:\n",
        "                    text = future.get(\"description\", str(future))\n",
        "                    metrics = evaluator.extract_metrics(text)\n",
        "                    evaluated_futures.append({\n",
        "                        \"text\": text,\n",
        "                        \"metrics\": metrics\n",
        "                    })\n",
        "\n",
        "                # 7. Prepare outputs: formatted text, visualization, and PDF report\n",
        "                display_text = format_display_output(modified_scenario, evaluated_futures)\n",
        "                plot_path = generate_visualization(evaluated_futures)\n",
        "                pdf_path = create_pdf_report(modified_scenario, evaluated_futures)\n",
        "\n",
        "                logger.info(\"Simulation completed successfully\")\n",
        "                return display_text, plot_path, pdf_path\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Simulation failed: {str(e)}\")\n",
        "                return (\n",
        "                    f\"Simulation Error: {str(e)}\\n\\nTry simplifying your scenario.\",\n",
        "                    None,\n",
        "                    None\n",
        "                )\n",
        "\n",
        "    return demo\n",
        "\n",
        "# ====================== RUN APP ======================\n",
        "if __name__ == \"__main__\":\n",
        "    interface = create_interface()\n",
        "    interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "AA3oyVcVMDRg",
        "outputId": "3c1cdf9d-74f1-4935-bb6a-aff0137050f8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://9d3bcf83ab8498756b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9d3bcf83ab8498756b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
